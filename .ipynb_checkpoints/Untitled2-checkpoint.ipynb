{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33f5dbe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T06:44:08.744368Z",
     "start_time": "2022-07-12T06:44:08.738997Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, torch, dgl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import check_files\n",
    "from data_ import *\n",
    "from train import *\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from turtle import forward\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn.pytorch as dglnn\n",
    "# from dgl.nn import Set2Set\n",
    "\n",
    "from torch_geometric.nn import SAGEConv, SAGPooling, Set2Set, GraphNorm, global_sort_pool, GlobalAttention\n",
    "from torch_geometric.utils import add_self_loops, subgraph\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa9652e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T06:32:07.426361Z",
     "start_time": "2022-07-12T06:31:56.960056Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/liangbilin/miniconda3/envs/pyg/lib/python3.8/site-packages/pandas/core/generic.py:5494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "omics_files=[\"./data/LGG/rna.csv.gz\", \"./data/LGG/met.csv.gz\", \"./data/LGG/miRNA_gene_mean.csv.gz\"]\n",
    "label_file = \"./data/LGG/label.csv\"\n",
    "add_file = None\n",
    "G, pathway, id_mapping = read_omics(omics_files=omics_files, \n",
    "                                    label_file=label_file, \n",
    "                                    add_file=add_file, \n",
    "                                    pathway_file='./Pathway/Rectome.pathway.tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86c5c084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T06:44:32.781667Z",
     "start_time": "2022-07-12T06:44:32.779641Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = G.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "12ae79ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T07:07:18.102223Z",
     "start_time": "2022-07-12T07:07:18.097690Z"
    }
   },
   "outputs": [],
   "source": [
    "class PathFeature(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(PathFeature, self).__init__()\n",
    "        # GNN-1\n",
    "        self.conv1 = SAGEConv(in_dim, in_dim)\n",
    "        self.pool1 = SAGPooling(in_dim, ratio=0.8)\n",
    "        self.readout1 = GlobalAttention(gate_nn=nn.Linear(in_dim, 1))\n",
    "        \n",
    "    def forward(self, g, h):\n",
    "        edge_index = g.edge_index\n",
    "        x = h\n",
    "        # GNN-1\n",
    "        x = torch.tanh(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, _, _, _ = self.pool1(x, edge_index, None, None)\n",
    "        x = self.readout1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "98209cf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T07:11:53.799552Z",
     "start_time": "2022-07-12T07:11:53.458164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1522, 0.1516, 0.1535, 0.1522, 0.1521, 0.1577, 0.1514, 0.1511, 0.1526,\n",
       "         0.1518, 0.1529, 0.1517, 0.1519, 0.1514, 0.1511, 0.1519, 0.1517, 0.1511,\n",
       "         0.1534, 0.1517, 0.1513, 0.1515, 0.1520, 0.1551, 0.1523, 0.1515, 0.1518,\n",
       "         0.1543, 0.1510, 0.1515, 0.1527, 0.1535, 0.1528, 0.1563, 0.1516, 0.1532,\n",
       "         0.1519, 0.1512, 0.1526, 0.1524, 0.1525, 0.1512, 0.1519, 0.1515, 0.1510,\n",
       "         0.1511, 0.1518, 0.1518, 0.1518, 0.1513, 0.1514, 0.1512, 0.1520, 0.1524,\n",
       "         0.1513, 0.1530, 0.1512, 0.1515, 0.1535, 0.1511, 0.1514, 0.1511, 0.1515,\n",
       "         0.1512, 0.1549, 0.1524, 0.1513, 0.1512, 0.1513, 0.1518, 0.1515, 0.1523,\n",
       "         0.1523, 0.1523, 0.1529, 0.1523, 0.1517, 0.1518, 0.1517, 0.1521, 0.1514,\n",
       "         0.1512, 0.1511, 0.1525, 0.1553, 0.1564, 0.1513, 0.1524, 0.1516, 0.1525,\n",
       "         0.1535, 0.1532, 0.1532, 0.1515, 0.1520, 0.1520, 0.1518, 0.1529, 0.1510,\n",
       "         0.1511, 0.1513, 0.1517, 0.1511, 0.1523, 0.1522, 0.1511, 0.1511, 0.1514,\n",
       "         0.1527, 0.1518, 0.1517, 0.1517, 0.1533, 0.1517, 0.1516]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeepMOI(nn.Module):\n",
    "    def __init__(self, in_dim, pathway, add_features=None):\n",
    "        \"\"\"\n",
    "        in_dim: == omics' number\n",
    "        hidden_dim: == \n",
    "        \"\"\"\n",
    "        super(DeepMOI, self).__init__()\n",
    "        self.pathway = pathway\n",
    "        \n",
    "        # Gene Layer\n",
    "        self.conv = SAGEConv(in_dim, in_dim, 'pool')\n",
    "                \n",
    "        # Pathway Layers     \n",
    "        self.submodels = nn.ModuleList()\n",
    "        for _ in range(len(self.pathway.pathway.unique())):\n",
    "            self.submodels.append(PathFeature(in_dim=in_dim))\n",
    "    \n",
    "        # MLP\n",
    "        self.lin = nn.Linear(3, 1)\n",
    "        self.mlp = nn.Sequential(\n",
    "                                 nn.Dropout(p=0.2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(len(self.pathway.pathway.unique()), 1),\n",
    "                                 nn.Sigmoid()\n",
    "                                )\n",
    "        self.norm = GraphNorm(in_dim*8)\n",
    "        \n",
    "        \n",
    "    def forward(self, g, h, c=None, output=False):\n",
    "        edge_index = g.edge_index\n",
    "        edge_index,_ = add_self_loops(edge_index=edge_index)\n",
    "        x = h\n",
    "        \n",
    "        # Gene Layer\n",
    "        x = torch.tanh(self.conv(x, edge_index))    \n",
    "        \n",
    "        # Pathway Layer\n",
    "        i = 0\n",
    "        readout = []\n",
    "        for path, group in self.pathway.groupby('pathway'):\n",
    "            nodes = list(set(group.src.to_list() + group.dest.to_list()))\n",
    "            sub_edge_idx,_ = subgraph(subset=nodes, edge_index=edge_index)\n",
    "            dat = Data(x=x, edge_index=sub_edge_idx)\n",
    "            out = self.submodels[i](dat, x)\n",
    "            readout.append(out)\n",
    "        readout = torch.cat(readout, dim=0)\n",
    "        readout = torch.tanh(self.lin(readout).T)\n",
    "        if output:\n",
    "            return readout\n",
    "        # MLP\n",
    "        logit = self.mlp(readout)\n",
    "\n",
    "        return logit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc4fe4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-12T07:17:37.008Z"
    }
   },
   "outputs": [],
   "source": [
    "model = DeepMOI(in_dim=3, pathway=pathway)\n",
    "model.eval()\n",
    "outputs = []\n",
    "for i in range(len(labels)):\n",
    "    print(i)\n",
    "    outputs.append(model(G, G.x[:, i, :], output=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e4b6ee95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T07:14:10.218530Z",
     "start_time": "2022-07-12T07:14:10.202001Z"
    }
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.nn.pytorch.factory import KNNGraph\n",
    "\n",
    "x = G.x.permute(1,0,2)\n",
    "kg = KNNGraph(10)\n",
    "sample_graph = kg(x[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7cd6ecbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T07:16:07.237342Z",
     "start_time": "2022-07-12T07:16:07.235231Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_graph.ndata['label'] = torch.tensor(labels, dtype=torch.long).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cf67869c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T07:15:59.058844Z",
     "start_time": "2022-07-12T07:15:59.048431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1df5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e15c7267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T07:07:49.209085Z",
     "start_time": "2022-07-12T07:07:36.747792Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5464]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5220]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5348]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5510]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5505]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5624]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5537]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5543]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5614]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5104]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5524]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5573]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5273]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5544]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5411]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5352]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8265/3308674328.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyg/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pyg/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "epoch = 10\n",
    "lr = 0.001\n",
    "minibatch=16\n",
    "device='cpu'\n",
    "\n",
    "train_idx, test_idx = data_split(labels=G.label, test_size=0.3)\n",
    "\n",
    "model = DeepMOI(in_dim=3, pathway=pathway)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.00001)\n",
    "\n",
    "# train model\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    model.train()\n",
    "    logits_epoch, labels_epoch, loss_epoch = [], [], [] # for training dataset evaluation\n",
    "    for idx in batch_idx(train_idx=train_idx, minibatch=minibatch):\n",
    "        logits_batch = []\n",
    "        for i in idx:\n",
    "            logit = model(G, G.x[:, i, :])\n",
    "            print(logit)\n",
    "            logits_batch.append(logit)\n",
    "            logits_epoch.append(logit.to(device='cpu').detach().numpy())\n",
    "        # backward\n",
    "        loss = nn.BCELoss()(torch.cat(logits_batch), torch.tensor(labels[idx], dtype=torch.float32, device=device).reshape(-1,1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_epoch.append(loss.item())\n",
    "        labels_epoch += idx\n",
    "\n",
    "    # evaluation for training dataset\n",
    "    logits_epoch = np.concatenate(logits_epoch)\n",
    "    labels_epoch = labels[train_idx]\n",
    "    loss_epoch = np.mean(loss_epoch)\n",
    "    acc, auc, f1_score_, sens, spec = evaluate(logits=logits_epoch, real_labels=labels_epoch)\n",
    "    print('Epoch {:2d} | Train_Loss {:.10f} | Train_ACC {:.3f} | Train_AUC {:.3f} | Train_F1_score {:.3f} | Train_Sens {:.3f} | Train_Spec {:.3f}'.format(\n",
    "        epoch, loss_epoch, acc, auc, f1_score_, sens, spec)\n",
    "        )\n",
    "    with open(os.path.join(outdir,'log.txt'), 'a') as F:\n",
    "        F.writelines('Epoch {:2d} | Train_Loss {:.10f} | Train_ACC {:.3f} | Train_AUC {:.3f} | Train_F1_score {:.3f} | Train_Sens {:.3f} | Train_Spec {:.3f}\\n'.format(\n",
    "        epoch, loss_epoch, acc, auc, f1_score_, sens, spec))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d2c5e5d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T07:08:25.023898Z",
     "start_time": "2022-07-12T07:08:25.021173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAGEConv(3, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
